/home/SageAttention_pr/csrc/qattn/rocm/attn_rocm.h -> /home/SageAttention_pr/csrc/qattn/rocm/attn_rocm.h [skipped, no changes]
/home/SageAttention_pr/csrc/qattn/rocm/pybind_rocm.cpp -> /home/SageAttention_pr/csrc/qattn/rocm/pybind_rocm.cpp [skipped, no changes]
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h -> /home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h [skipped, no changes]
/home/SageAttention_pr/csrc/qattn/rocm/args.hpp -> /home/SageAttention_pr/csrc/qattn/rocm/args.hpp [skipped, no changes]
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.cu -> /home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip [skipped, already hipified]
/home/SageAttention_pr/csrc/utils.cuh -> /home/SageAttention_pr/csrc/utils.cuh [skipped, no changes]
/home/SageAttention_pr/csrc/qattn/rocm/attn_utils.h -> /home/SageAttention_pr/csrc/qattn/rocm/attn_utils_hip.h [skipped, already hipified]
/home/SageAttention_pr/csrc/qattn/rocm/sgattn.cu -> /home/SageAttention_pr/csrc/qattn/rocm/sgattn.hip [ok]
Total number of unsupported CUDA function calls: 0


Total number of replaced kernel launches: 0
/home/SageAttention_pr/csrc/fused/rocm/fused.h -> /home/SageAttention_pr/csrc/fused/rocm/fused.h [skipped, no changes]
/home/SageAttention_pr/csrc/fused/rocm/pybind_rocm.cpp -> /home/SageAttention_pr/csrc/fused/rocm/pybind_rocm.cpp [skipped, no changes]
/home/SageAttention_pr/csrc/fused/rocm/dispatch_utils.h -> /home/SageAttention_pr/csrc/fused/rocm/dispatch_utils.h [skipped, no changes]
/home/SageAttention_pr/csrc/reduction_utils.h -> /home/SageAttention_pr/csrc/reduction_utils_hip.h [skipped, already hipified]
/home/SageAttention_pr/csrc/fused/rocm/fused.cu -> /home/SageAttention_pr/csrc/fused/rocm/fused.hip [skipped, already hipified]
Total number of unsupported CUDA function calls: 0


Total number of replaced kernel launches: 2
Current ROCm architecture detected: gfx942
running clean
removing 'build/temp.linux-x86_64-cpython-312' (and everything under it)
[H[2J[3J/home/SageAttention_pr/csrc/qattn/rocm/attn_rocm.h -> /home/SageAttention_pr/csrc/qattn/rocm/attn_rocm.h [skipped, no changes]
/home/SageAttention_pr/csrc/qattn/rocm/pybind_rocm.cpp -> /home/SageAttention_pr/csrc/qattn/rocm/pybind_rocm.cpp [skipped, no changes]
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h -> /home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h [skipped, no changes]
/home/SageAttention_pr/csrc/qattn/rocm/args.hpp -> /home/SageAttention_pr/csrc/qattn/rocm/args.hpp [skipped, no changes]
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.cu -> /home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip [skipped, already hipified]
/home/SageAttention_pr/csrc/utils.cuh -> /home/SageAttention_pr/csrc/utils.cuh [skipped, no changes]
/home/SageAttention_pr/csrc/qattn/rocm/attn_utils.h -> /home/SageAttention_pr/csrc/qattn/rocm/attn_utils_hip.h [skipped, already hipified]
/home/SageAttention_pr/csrc/qattn/rocm/sgattn.cu -> /home/SageAttention_pr/csrc/qattn/rocm/sgattn.hip [skipped, already hipified]
Total number of unsupported CUDA function calls: 0


Total number of replaced kernel launches: 0
/home/SageAttention_pr/csrc/fused/rocm/fused.h -> /home/SageAttention_pr/csrc/fused/rocm/fused.h [skipped, no changes]
/home/SageAttention_pr/csrc/fused/rocm/pybind_rocm.cpp -> /home/SageAttention_pr/csrc/fused/rocm/pybind_rocm.cpp [skipped, no changes]
/home/SageAttention_pr/csrc/fused/rocm/dispatch_utils.h -> /home/SageAttention_pr/csrc/fused/rocm/dispatch_utils.h [skipped, no changes]
/home/SageAttention_pr/csrc/reduction_utils.h -> /home/SageAttention_pr/csrc/reduction_utils_hip.h [skipped, already hipified]
/home/SageAttention_pr/csrc/fused/rocm/fused.cu -> /home/SageAttention_pr/csrc/fused/rocm/fused.hip [skipped, already hipified]
Total number of unsupported CUDA function calls: 0


Total number of replaced kernel launches: 2
Current ROCm architecture detected: gfx942
running install
running build
running build_py
creating build/lib.linux-x86_64-cpython-312/sageattention
copying sageattention/__init__.py -> build/lib.linux-x86_64-cpython-312/sageattention
copying sageattention/sm80_compile.py -> build/lib.linux-x86_64-cpython-312/sageattention
copying sageattention/quant.py -> build/lib.linux-x86_64-cpython-312/sageattention
copying sageattention/sm89_compile.py -> build/lib.linux-x86_64-cpython-312/sageattention
copying sageattention/sm90_compile.py -> build/lib.linux-x86_64-cpython-312/sageattention
copying sageattention/core.py -> build/lib.linux-x86_64-cpython-312/sageattention
copying sageattention/fa3_wrapper.py -> build/lib.linux-x86_64-cpython-312/sageattention
creating build/lib.linux-x86_64-cpython-312/sageattention/triton
copying sageattention/triton/attn_qk_int8_per_block_causal.py -> build/lib.linux-x86_64-cpython-312/sageattention/triton
copying sageattention/triton/quant_per_thread.py -> build/lib.linux-x86_64-cpython-312/sageattention/triton
copying sageattention/triton/quant_per_block_varlen.py -> build/lib.linux-x86_64-cpython-312/sageattention/triton
copying sageattention/triton/__init__.py -> build/lib.linux-x86_64-cpython-312/sageattention/triton
copying sageattention/triton/attn_qk_int8_per_block_causal_varlen.py -> build/lib.linux-x86_64-cpython-312/sageattention/triton
copying sageattention/triton/attn_qk_int8_per_block.py -> build/lib.linux-x86_64-cpython-312/sageattention/triton
copying sageattention/triton/attn_qk_int8_block_varlen.py -> build/lib.linux-x86_64-cpython-312/sageattention/triton
copying sageattention/triton/quant_per_block.py -> build/lib.linux-x86_64-cpython-312/sageattention/triton
running build_ext
building 'sageattention._qattn_rocm' extension
creating /home/SageAttention_pr/build/temp.linux-x86_64-cpython-312/csrc/qattn/rocm
[1/3] c++ -MMD -MF /home/SageAttention_pr/build/temp.linux-x86_64-cpython-312/csrc/qattn/rocm/pybind_rocm.o.d -fno-strict-overflow -Wsign-compare -DNDEBUG -g -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/opt/rocm/include -I/opt/rocm/include/hip -I/usr/local/lib/python3.12/dist-packages/torch/include -I/usr/local/lib/python3.12/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.12/dist-packages/torch/include/TH -I/usr/local/lib/python3.12/dist-packages/torch/include/THC -I/usr/local/lib/python3.12/dist-packages/torch/include/THH -I/opt/rocm-6.3.0/include -I/usr/include/python3.12 -c -c /home/SageAttention_pr/csrc/qattn/rocm/pybind_rocm.cpp -o /home/SageAttention_pr/build/temp.linux-x86_64-cpython-312/csrc/qattn/rocm/pybind_rocm.o -fPIC -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -std=c++17 -D_GLIBCXX_USE_CXX11_ABI=1 -DUSE_ROCM=1 -O3 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1016"' -DTORCH_EXTENSION_NAME=_qattn_rocm -D_GLIBCXX_USE_CXX11_ABI=1
[2/3] /opt/rocm-6.3.0/bin/hipcc  -I/opt/rocm/include -I/opt/rocm/include/hip -I/usr/local/lib/python3.12/dist-packages/torch/include -I/usr/local/lib/python3.12/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.12/dist-packages/torch/include/TH -I/usr/local/lib/python3.12/dist-packages/torch/include/THC -I/usr/local/lib/python3.12/dist-packages/torch/include/THH -I/opt/rocm-6.3.0/include -I/usr/include/python3.12 -c -c /home/SageAttention_pr/csrc/qattn/rocm/sgattn.hip -o /home/SageAttention_pr/build/temp.linux-x86_64-cpython-312/csrc/qattn/rocm/sgattn.o -fPIC -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -std=c++17 -D_GLIBCXX_USE_CXX11_ABI=1 -DUSE_ROCM=1 -O3 --offload-arch=gfx942 -D__ROCM_ARCH_GFX942 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1016"' -DTORCH_EXTENSION_NAME=_qattn_rocm -D_GLIBCXX_USE_CXX11_ABI=1 -fno-gpu-rdc
[3/3] /opt/rocm-6.3.0/bin/hipcc  -I/opt/rocm/include -I/opt/rocm/include/hip -I/usr/local/lib/python3.12/dist-packages/torch/include -I/usr/local/lib/python3.12/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.12/dist-packages/torch/include/TH -I/usr/local/lib/python3.12/dist-packages/torch/include/THC -I/usr/local/lib/python3.12/dist-packages/torch/include/THH -I/opt/rocm-6.3.0/include -I/usr/include/python3.12 -c -c /home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip -o /home/SageAttention_pr/build/temp.linux-x86_64-cpython-312/csrc/qattn/rocm/launch_sgattn.o -fPIC -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -std=c++17 -D_GLIBCXX_USE_CXX11_ABI=1 -DUSE_ROCM=1 -O3 --offload-arch=gfx942 -D__ROCM_ARCH_GFX942 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1016"' -DTORCH_EXTENSION_NAME=_qattn_rocm -D_GLIBCXX_USE_CXX11_ABI=1 -fno-gpu-rdc
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:78:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   78 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:65:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   65 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:52:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   52 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:39:5: note: expanded from macro 'DISPATCH_CAUSAL'
   39 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:26:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   26 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:81:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   81 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:65:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   65 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:52:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   52 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:39:5: note: expanded from macro 'DISPATCH_CAUSAL'
   39 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:26:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   26 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:78:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   78 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:68:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   68 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:52:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   52 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:39:5: note: expanded from macro 'DISPATCH_CAUSAL'
   39 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:26:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   26 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:81:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   81 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:68:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   68 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:52:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   52 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:39:5: note: expanded from macro 'DISPATCH_CAUSAL'
   39 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:26:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   26 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:78:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   78 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:65:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   65 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:55:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   55 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:39:5: note: expanded from macro 'DISPATCH_CAUSAL'
   39 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:26:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   26 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:81:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   81 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:65:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   65 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:55:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   55 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:39:5: note: expanded from macro 'DISPATCH_CAUSAL'
   39 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:26:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   26 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:78:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   78 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:68:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   68 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:55:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   55 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:39:5: note: expanded from macro 'DISPATCH_CAUSAL'
   39 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:26:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   26 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:81:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   81 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:68:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   68 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:55:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   55 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:39:5: note: expanded from macro 'DISPATCH_CAUSAL'
   39 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:26:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   26 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:78:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   78 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:65:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   65 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:52:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   52 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:42:5: note: expanded from macro 'DISPATCH_CAUSAL'
   42 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:26:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   26 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:81:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   81 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:65:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   65 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:52:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   52 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:42:5: note: expanded from macro 'DISPATCH_CAUSAL'
   42 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:26:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   26 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:78:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   78 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:68:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   68 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:52:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   52 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:42:5: note: expanded from macro 'DISPATCH_CAUSAL'
   42 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:26:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   26 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:81:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   81 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:68:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   68 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:52:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   52 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:42:5: note: expanded from macro 'DISPATCH_CAUSAL'
   42 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:26:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   26 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:78:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   78 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:65:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   65 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:55:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   55 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:42:5: note: expanded from macro 'DISPATCH_CAUSAL'
   42 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:26:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   26 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:81:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   81 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:65:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   65 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:55:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   55 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:42:5: note: expanded from macro 'DISPATCH_CAUSAL'
   42 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:26:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   26 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:78:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   78 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:68:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   68 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:55:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   55 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:42:5: note: expanded from macro 'DISPATCH_CAUSAL'
   42 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:26:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   26 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:81:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   81 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:68:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   68 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:55:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   55 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:42:5: note: expanded from macro 'DISPATCH_CAUSAL'
   42 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:26:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   26 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:78:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   78 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:65:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   65 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:52:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   52 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:39:5: note: expanded from macro 'DISPATCH_CAUSAL'
   39 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:29:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   29 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:81:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   81 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:65:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   65 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:52:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   52 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:39:5: note: expanded from macro 'DISPATCH_CAUSAL'
   39 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:29:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   29 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:78:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   78 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:68:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   68 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:52:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   52 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:39:5: note: expanded from macro 'DISPATCH_CAUSAL'
   39 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:29:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   29 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:81:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   81 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:68:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   68 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:52:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   52 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:39:5: note: expanded from macro 'DISPATCH_CAUSAL'
   39 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:29:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   29 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:78:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   78 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:65:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   65 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:55:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   55 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:39:5: note: expanded from macro 'DISPATCH_CAUSAL'
   39 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:29:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   29 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:81:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   81 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:65:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   65 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:55:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   55 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:39:5: note: expanded from macro 'DISPATCH_CAUSAL'
   39 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:29:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   29 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:78:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   78 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:68:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   68 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:55:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   55 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:39:5: note: expanded from macro 'DISPATCH_CAUSAL'
   39 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:29:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   29 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:81:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   81 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:68:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   68 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:55:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   55 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:39:5: note: expanded from macro 'DISPATCH_CAUSAL'
   39 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:29:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   29 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:78:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   78 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:65:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   65 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:52:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   52 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:42:5: note: expanded from macro 'DISPATCH_CAUSAL'
   42 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:29:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   29 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:81:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   81 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:65:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   65 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:52:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   52 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:42:5: note: expanded from macro 'DISPATCH_CAUSAL'
   42 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:29:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   29 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:78:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   78 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:68:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   68 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:52:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   52 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:42:5: note: expanded from macro 'DISPATCH_CAUSAL'
   42 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:29:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   29 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:81:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   81 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:68:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   68 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:52:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   52 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:42:5: note: expanded from macro 'DISPATCH_CAUSAL'
   42 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:29:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   29 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:78:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   78 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:65:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   65 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:55:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   55 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:42:5: note: expanded from macro 'DISPATCH_CAUSAL'
   42 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:29:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   29 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:81:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   81 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:65:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   65 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:55:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   55 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:42:5: note: expanded from macro 'DISPATCH_CAUSAL'
   42 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:29:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   29 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:78:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   78 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:68:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   68 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:55:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   55 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:42:5: note: expanded from macro 'DISPATCH_CAUSAL'
   42 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:29:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   29 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:81:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   81 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:68:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   68 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:55:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   55 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:42:5: note: expanded from macro 'DISPATCH_CAUSAL'
   42 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:29:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   29 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
32 warnings generated when compiling for gfx942.
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:78:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   78 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:65:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   65 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:52:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   52 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:39:5: note: expanded from macro 'DISPATCH_CAUSAL'
   39 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:26:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   26 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:81:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   81 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:65:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   65 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:52:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   52 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:39:5: note: expanded from macro 'DISPATCH_CAUSAL'
   39 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:26:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   26 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:78:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   78 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:68:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   68 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:52:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   52 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:39:5: note: expanded from macro 'DISPATCH_CAUSAL'
   39 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:26:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   26 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:81:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   81 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:68:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   68 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:52:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   52 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:39:5: note: expanded from macro 'DISPATCH_CAUSAL'
   39 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:26:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   26 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:78:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   78 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:65:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   65 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:55:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   55 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:39:5: note: expanded from macro 'DISPATCH_CAUSAL'
   39 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:26:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   26 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:81:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   81 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:65:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   65 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:55:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   55 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:39:5: note: expanded from macro 'DISPATCH_CAUSAL'
   39 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:26:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   26 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:78:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   78 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:68:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   68 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:55:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   55 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:39:5: note: expanded from macro 'DISPATCH_CAUSAL'
   39 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:26:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   26 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:81:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   81 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:68:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   68 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:55:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   55 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:39:5: note: expanded from macro 'DISPATCH_CAUSAL'
   39 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:26:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   26 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:78:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   78 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:65:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   65 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:52:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   52 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:42:5: note: expanded from macro 'DISPATCH_CAUSAL'
   42 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:26:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   26 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:81:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   81 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:65:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   65 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:52:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   52 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:42:5: note: expanded from macro 'DISPATCH_CAUSAL'
   42 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:26:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   26 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:78:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   78 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:68:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   68 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:52:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   52 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:42:5: note: expanded from macro 'DISPATCH_CAUSAL'
   42 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:26:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   26 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:81:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   81 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:68:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   68 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:52:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   52 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:42:5: note: expanded from macro 'DISPATCH_CAUSAL'
   42 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:26:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   26 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:78:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   78 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:65:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   65 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:55:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   55 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:42:5: note: expanded from macro 'DISPATCH_CAUSAL'
   42 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:26:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   26 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:81:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   81 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:65:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   65 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:55:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   55 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:42:5: note: expanded from macro 'DISPATCH_CAUSAL'
   42 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:26:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   26 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:78:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   78 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:68:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   68 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:55:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   55 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:42:5: note: expanded from macro 'DISPATCH_CAUSAL'
   42 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:26:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   26 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:81:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   81 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:68:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   68 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:55:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   55 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:42:5: note: expanded from macro 'DISPATCH_CAUSAL'
   42 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:26:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   26 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:78:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   78 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:65:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   65 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:52:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   52 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:39:5: note: expanded from macro 'DISPATCH_CAUSAL'
   39 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:29:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   29 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:81:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   81 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:65:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   65 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:52:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   52 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:39:5: note: expanded from macro 'DISPATCH_CAUSAL'
   39 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:29:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   29 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:78:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   78 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:68:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   68 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:52:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   52 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:39:5: note: expanded from macro 'DISPATCH_CAUSAL'
   39 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:29:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   29 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:81:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   81 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:68:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   68 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:52:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   52 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:39:5: note: expanded from macro 'DISPATCH_CAUSAL'
   39 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:29:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   29 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:78:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   78 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:65:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   65 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:55:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   55 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:39:5: note: expanded from macro 'DISPATCH_CAUSAL'
   39 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:29:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   29 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:81:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   81 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:65:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   65 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:55:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   55 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:39:5: note: expanded from macro 'DISPATCH_CAUSAL'
   39 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:29:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   29 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:78:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   78 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:68:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   68 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:55:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   55 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:39:5: note: expanded from macro 'DISPATCH_CAUSAL'
   39 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:29:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   29 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:81:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   81 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:68:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   68 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:55:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   55 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:39:5: note: expanded from macro 'DISPATCH_CAUSAL'
   39 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:29:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   29 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:78:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   78 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:65:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   65 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:52:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   52 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:42:5: note: expanded from macro 'DISPATCH_CAUSAL'
   42 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:29:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   29 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:81:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   81 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:65:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   65 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:52:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   52 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:42:5: note: expanded from macro 'DISPATCH_CAUSAL'
   42 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:29:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   29 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:78:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   78 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:68:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   68 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:52:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   52 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:42:5: note: expanded from macro 'DISPATCH_CAUSAL'
   42 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:29:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   29 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:81:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   81 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:68:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   68 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:52:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   52 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:42:5: note: expanded from macro 'DISPATCH_CAUSAL'
   42 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:29:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   29 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:78:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   78 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:65:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   65 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:55:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   55 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:42:5: note: expanded from macro 'DISPATCH_CAUSAL'
   42 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:29:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   29 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:81:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   81 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:65:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   65 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:55:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   55 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:42:5: note: expanded from macro 'DISPATCH_CAUSAL'
   42 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:29:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   29 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:78:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   78 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:68:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   68 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:55:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   55 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:42:5: note: expanded from macro 'DISPATCH_CAUSAL'
   42 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:29:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   29 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/launch_sgattn.hip:227:25: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]
  169 |                         //     TBLOCK_Y  = gfx9Params::TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  170 |                         //     WARP_SIZE = gfx9Params::WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  171 |                         // }
      |                         ~~~~
  172 |                         const size_t Mx = (size_t)ROCWMMA_M * BLOCKS_X * TBLOCK_X / WARP_SIZE;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  173 |                         const size_t My = (size_t)ROCWMMA_N * BLOCKS_Y * TBLOCK_Y;
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  174 |                         size_t smem_max = ::max(2u * sizeof(int8_t) * (Mx + My) * ROCWMMA_K, 1u * sizeof(float) * Mx * My);
      |                         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  175 | 
  176 |                         hipFuncSetAttribute(
      |                         ^~~~~~~~~~~~~~~~~~~~
  177 |                             (const void*)qk_int_sv_f8_attn_kernel<CTA_Q, CTA_K, WARP_Q, WARP_K, HEAD_DIM, DataType::kInt8,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  178 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  179 |                                                                 static_cast<QuantGranularity>(QK_QUANT_GRAN),
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  180 |                                                                 1, float, false, DTypeOut,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~
  181 |                                                                 ComputeUnit::kCudaCore, mask_mode,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  182 |                                                                 RETURN_LSE, false, false, false>,
      |                                                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  183 |                             hipFuncAttributeMaxDynamicSharedMemorySize,
      |                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  184 |                             (int)smem_max);
      |                             ~~~~~~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:81:5: note: expanded from macro 'DISPATCH_PYTORCH_DTYPE_TO_CTYPE_FP16'
   81 |     __VA_ARGS__                                                                         \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:68:5: note: expanded from macro 'DISPATCH_RETURN_LSE'
   68 |     __VA_ARGS__                                                  \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:55:5: note: expanded from macro 'DISPATCH_QK_QUANT_GRAN'
   55 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:42:5: note: expanded from macro 'DISPATCH_CAUSAL'
   42 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
/home/SageAttention_pr/csrc/qattn/rocm/dispatch_utils.h:29:5: note: expanded from macro 'DISPATCH_HEAD_DIM'
   29 |     __VA_ARGS__                                                 \
      |     ^~~~~~~~~~~
32 warnings generated when compiling for host.
x86_64-linux-gnu-g++ -fno-strict-overflow -Wsign-compare -DNDEBUG -g -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 /home/SageAttention_pr/build/temp.linux-x86_64-cpython-312/csrc/qattn/rocm/launch_sgattn.o /home/SageAttention_pr/build/temp.linux-x86_64-cpython-312/csrc/qattn/rocm/pybind_rocm.o /home/SageAttention_pr/build/temp.linux-x86_64-cpython-312/csrc/qattn/rocm/sgattn.o -L/opt/rocm/lib -L/opt/rocm/lib64 -L/usr/local/lib/python3.12/dist-packages/torch/lib -L/usr/local/lib/python3.12/dist-packages/torch/lib -L/opt/rocm-6.3.0/lib -L/opt/rocm-6.3.0/hip/lib -L/usr/lib/x86_64-linux-gnu -Wl,--enable-new-dtags,-rpath,/opt/rocm/lib -Wl,--enable-new-dtags,-rpath,/opt/rocm/lib64 -Wl,--enable-new-dtags,-rpath,/usr/local/lib/python3.12/dist-packages/torch/lib -lamdhip64 -lhiprtc -lrocblas -lhipblas -lc10 -ltorch -ltorch_python -lc10 -ltorch -ltorch_cpu -ltorch_python -lamdhip64 -lc10_hip -ltorch_hip -o build/lib.linux-x86_64-cpython-312/sageattention/_qattn_rocm.cpython-312-x86_64-linux-gnu.so
building 'sageattention._fused' extension
creating /home/SageAttention_pr/build/temp.linux-x86_64-cpython-312/csrc/fused/rocm
[1/2] c++ -MMD -MF /home/SageAttention_pr/build/temp.linux-x86_64-cpython-312/csrc/fused/rocm/pybind_rocm.o.d -fno-strict-overflow -Wsign-compare -DNDEBUG -g -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/opt/rocm/include -I/opt/rocm/include/hip -I/usr/local/lib/python3.12/dist-packages/torch/include -I/usr/local/lib/python3.12/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.12/dist-packages/torch/include/TH -I/usr/local/lib/python3.12/dist-packages/torch/include/THC -I/usr/local/lib/python3.12/dist-packages/torch/include/THH -I/opt/rocm-6.3.0/include -I/usr/include/python3.12 -c -c /home/SageAttention_pr/csrc/fused/rocm/pybind_rocm.cpp -o /home/SageAttention_pr/build/temp.linux-x86_64-cpython-312/csrc/fused/rocm/pybind_rocm.o -fPIC -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -fPIC -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -std=c++17 -D_GLIBCXX_USE_CXX11_ABI=1 -DUSE_ROCM=1 -O3 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1016"' -DTORCH_EXTENSION_NAME=_fused -D_GLIBCXX_USE_CXX11_ABI=1
[2/2] /opt/rocm-6.3.0/bin/hipcc  -I/opt/rocm/include -I/opt/rocm/include/hip -I/usr/local/lib/python3.12/dist-packages/torch/include -I/usr/local/lib/python3.12/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.12/dist-packages/torch/include/TH -I/usr/local/lib/python3.12/dist-packages/torch/include/THC -I/usr/local/lib/python3.12/dist-packages/torch/include/THH -I/opt/rocm-6.3.0/include -I/usr/include/python3.12 -c -c /home/SageAttention_pr/csrc/fused/rocm/fused.hip -o /home/SageAttention_pr/build/temp.linux-x86_64-cpython-312/csrc/fused/rocm/fused.o -fPIC -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -std=c++17 -D_GLIBCXX_USE_CXX11_ABI=1 -DUSE_ROCM=1 -O3 --offload-arch=gfx942 -D__ROCM_ARCH_GFX942 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE="_gcc"' '-DPYBIND11_STDLIB="_libstdcpp"' '-DPYBIND11_BUILD_ABI="_cxxabi1016"' -DTORCH_EXTENSION_NAME=_fused -D_GLIBCXX_USE_CXX11_ABI=1 -fno-gpu-rdc
/home/SageAttention_pr/csrc/fused/rocm/fused.hip:18:9: warning: #pragma once in main file [-Wpragma-once-outside-header]
   18 | #pragma once
      |         ^
1 warning generated when compiling for gfx942.
/home/SageAttention_pr/csrc/fused/rocm/fused.hip:18:9: warning: #pragma once in main file [-Wpragma-once-outside-header]
   18 | #pragma once
      |         ^
1 warning generated when compiling for host.
x86_64-linux-gnu-g++ -fno-strict-overflow -Wsign-compare -DNDEBUG -g -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 /home/SageAttention_pr/build/temp.linux-x86_64-cpython-312/csrc/fused/rocm/fused.o /home/SageAttention_pr/build/temp.linux-x86_64-cpython-312/csrc/fused/rocm/pybind_rocm.o -L/opt/rocm/lib -L/opt/rocm/lib64 -L/usr/local/lib/python3.12/dist-packages/torch/lib -L/usr/local/lib/python3.12/dist-packages/torch/lib -L/opt/rocm-6.3.0/lib -L/opt/rocm-6.3.0/hip/lib -L/usr/lib/x86_64-linux-gnu -Wl,--enable-new-dtags,-rpath,/opt/rocm/lib -Wl,--enable-new-dtags,-rpath,/opt/rocm/lib64 -Wl,--enable-new-dtags,-rpath,/usr/local/lib/python3.12/dist-packages/torch/lib -lamdhip64 -lhiprtc -lrocblas -lhipblas -lc10 -ltorch -ltorch_python -lc10 -ltorch -ltorch_cpu -ltorch_python -lamdhip64 -lc10_hip -ltorch_hip -o build/lib.linux-x86_64-cpython-312/sageattention/_fused.cpython-312-x86_64-linux-gnu.so
running install_lib
copying build/lib.linux-x86_64-cpython-312/sageattention/_fused.cpython-312-x86_64-linux-gnu.so -> /usr/local/lib/python3.12/dist-packages/sageattention
copying build/lib.linux-x86_64-cpython-312/sageattention/_qattn_rocm.cpython-312-x86_64-linux-gnu.so -> /usr/local/lib/python3.12/dist-packages/sageattention
running install_egg_info
running egg_info
writing sageattention.egg-info/PKG-INFO
writing dependency_links to sageattention.egg-info/dependency_links.txt
writing top-level names to sageattention.egg-info/top_level.txt
reading manifest template 'MANIFEST.in'
adding license file 'LICENSE'
writing manifest file 'sageattention.egg-info/SOURCES.txt'
removing '/usr/local/lib/python3.12/dist-packages/sageattention-2.2.0-py3.12.egg-info' (and everything under it)
Copying sageattention.egg-info to /usr/local/lib/python3.12/dist-packages/sageattention-2.2.0-py3.12.egg-info
running install_scripts
